{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMw1IJiiiWlF",
        "outputId": "1a13098e-26c3-4fef-bf07-db881b658825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dataset\n",
        "documents = [\n",
        "    \"I love programming in Python.\",\n",
        "    \"Python is a great language for machine learning.\",\n",
        "    \"Machine learning and deep learning are exciting fields.\",\n",
        "    \"Deep learning is a subset of machine learning.\"\n",
        "]"
      ],
      "metadata": {
        "id": "AXSgrGzTjNvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 1. Bag-of-Words (BoW)\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(documents)\n",
        "bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(\"Bag-of-Words Representation:\")\n",
        "print(bow_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_dsw-OCjNru",
        "outputId": "6beb6d97-3fd2-4e22-d672-fcef2fa5d660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words Representation:\n",
            "   and  are  deep  exciting  fields  for  great  in  is  language  learning  \\\n",
            "0    0    0     0         0       0    0      0   1   0         0         0   \n",
            "1    0    0     0         0       0    1      1   0   1         1         1   \n",
            "2    1    1     1         1       1    0      0   0   0         0         2   \n",
            "3    0    0     1         0       0    0      0   0   1         0         2   \n",
            "\n",
            "   love  machine  of  programming  python  subset  \n",
            "0     1        0   0            1       1       0  \n",
            "1     0        1   0            0       1       0  \n",
            "2     0        1   0            0       0       0  \n",
            "3     0        1   1            0       0       1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 2. TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(documents)\n",
        "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer_tfidf.get_feature_names_out())\n",
        "print(\"\\nTF-IDF Representation:\")\n",
        "print(tfidf_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qly4ssNtjNkL",
        "outputId": "16981083-f057-4509-a000-bcadcb229e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Representation:\n",
            "        and       are      deep  exciting    fields       for     great  \\\n",
            "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.444642  0.444642   \n",
            "2  0.387532  0.387532  0.305534  0.387532  0.387532  0.000000  0.000000   \n",
            "3  0.000000  0.000000  0.343104  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "         in        is  language  learning      love   machine        of  \\\n",
            "0  0.525473  0.000000  0.000000  0.000000  0.525473  0.000000  0.000000   \n",
            "1  0.000000  0.350561  0.444642  0.283809  0.000000  0.283809  0.000000   \n",
            "2  0.000000  0.000000  0.000000  0.494713  0.000000  0.247356  0.000000   \n",
            "3  0.000000  0.343104  0.000000  0.555545  0.000000  0.277773  0.435184   \n",
            "\n",
            "   programming    python    subset  \n",
            "0     0.525473  0.414289  0.000000  \n",
            "1     0.000000  0.350561  0.000000  \n",
            "2     0.000000  0.000000  0.000000  \n",
            "3     0.000000  0.000000  0.435184  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 3. Word2Vec\n",
        "# Tokenize sentences\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]"
      ],
      "metadata": {
        "id": "tEKBB-SDjdN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "f9sSNGnNjdKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Get vector for a word\n",
        "word = \"python\"\n",
        "if word in word2vec_model.wv:\n",
        "    print(f\"\\nWord2Vec representation for '{word}':\")\n",
        "    print(word2vec_model.wv[word])\n",
        "else:\n",
        "    print(f\"\\nWord '{word}' not in vocabulary.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD4GquCHjdHb",
        "outputId": "1c1e9af5-6dd4-46d4-c581-53d72fe1208e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word2Vec representation for 'python':\n",
            "[-8.7294905e-03  2.1309664e-03 -8.7221497e-04 -9.3176709e-03\n",
            " -9.4262864e-03 -1.4121713e-03  4.4327485e-03  3.7056087e-03\n",
            " -6.4993422e-03 -6.8745553e-03 -4.9994662e-03 -2.2889308e-03\n",
            " -7.2515626e-03 -9.6015455e-03 -2.7428993e-03 -8.3611486e-03\n",
            " -6.0372930e-03 -5.6691379e-03 -2.3450621e-03 -1.7070804e-03\n",
            " -8.9564817e-03 -7.3632010e-04  8.1544742e-03  7.6879794e-03\n",
            " -7.2045443e-03 -3.6662007e-03  3.1174424e-03 -9.5695946e-03\n",
            "  1.4760050e-03  6.5260385e-03  5.7487735e-03 -8.7641701e-03\n",
            " -4.5173578e-03 -8.1414981e-03  4.6832771e-05  9.2643471e-03\n",
            "  5.9750592e-03  5.0687077e-03  5.0633666e-03 -3.2406410e-03\n",
            "  9.5541235e-03 -7.3580840e-03 -7.2726533e-03 -2.2655146e-03\n",
            " -7.7928969e-04 -3.2141213e-03 -5.9111224e-04  7.4884510e-03\n",
            " -6.9720595e-04 -1.6244808e-03  2.7462984e-03 -8.3614178e-03\n",
            "  7.8558167e-03  8.5369283e-03 -9.5843021e-03  2.4482766e-03\n",
            "  9.9071162e-03 -7.6642046e-03 -6.9670668e-03 -7.7347141e-03\n",
            "  8.3938064e-03 -6.8061694e-04  9.1432426e-03 -8.1595331e-03\n",
            "  3.7438897e-03  2.6363563e-03  7.4452185e-04  2.3262349e-03\n",
            " -7.4673374e-03 -9.3567520e-03  2.3536938e-03  6.1463714e-03\n",
            "  7.9870848e-03  5.7374481e-03 -7.7760662e-04  8.3044562e-03\n",
            " -9.3380176e-03  3.4054723e-03  2.6796857e-04  3.8563926e-03\n",
            "  7.3834900e-03 -6.7241895e-03  5.5855936e-03 -9.5238341e-03\n",
            " -8.0419373e-04 -8.6892769e-03 -5.0987443e-03  9.2867622e-03\n",
            " -1.8576009e-03  2.9133360e-03  9.0714693e-03  8.9376858e-03\n",
            " -8.2079172e-03 -3.0142153e-03  9.8859984e-03  5.1051178e-03\n",
            " -1.5868659e-03 -8.6925747e-03  2.9592637e-03 -6.6748913e-03]\n"
          ]
        }
      ]
    }
  ]
}